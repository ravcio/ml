{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction of  'Credit_card', 'Savings', 'Morgage' in File2.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "HTML('''<script>\n",
    "code_show=true; \n",
    "function code_toggle() {\n",
    " if (code_show){\n",
    " $('div.input').hide();\n",
    " } else {\n",
    " $('div.input').show();\n",
    " }\n",
    " code_show = !code_show\n",
    "} \n",
    "$( document ).ready(code_toggle);\n",
    "</script>\n",
    "<form action=\"javascript:code_toggle()\"><input type=\"submit\" \n",
    "value=\"Click here to toggle on/off the raw code.\"></form>''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'File2.zip'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-3-4580b702ce7d>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0mfn\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34mr'File2.zip'\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0mdf\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mpd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mread_csv\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfn\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msep\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m'|'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mencoding\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m'cp1252'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      3\u001B[0m \u001B[0mdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcolumns\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0mdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/ml/.venv/lib/python3.8/site-packages/pandas/io/parsers.py\u001B[0m in \u001B[0;36mread_csv\u001B[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001B[0m\n\u001B[1;32m    608\u001B[0m     \u001B[0mkwds\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mupdate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mkwds_defaults\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    609\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 610\u001B[0;31m     \u001B[0;32mreturn\u001B[0m \u001B[0m_read\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfilepath_or_buffer\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkwds\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    611\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    612\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/ml/.venv/lib/python3.8/site-packages/pandas/io/parsers.py\u001B[0m in \u001B[0;36m_read\u001B[0;34m(filepath_or_buffer, kwds)\u001B[0m\n\u001B[1;32m    460\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    461\u001B[0m     \u001B[0;31m# Create the parser.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 462\u001B[0;31m     \u001B[0mparser\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mTextFileReader\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfilepath_or_buffer\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwds\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    463\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    464\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mchunksize\u001B[0m \u001B[0;32mor\u001B[0m \u001B[0miterator\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/ml/.venv/lib/python3.8/site-packages/pandas/io/parsers.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, f, engine, **kwds)\u001B[0m\n\u001B[1;32m    817\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0moptions\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"has_index_names\"\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mkwds\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"has_index_names\"\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    818\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 819\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_engine\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_make_engine\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mengine\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    820\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    821\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mclose\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/ml/.venv/lib/python3.8/site-packages/pandas/io/parsers.py\u001B[0m in \u001B[0;36m_make_engine\u001B[0;34m(self, engine)\u001B[0m\n\u001B[1;32m   1048\u001B[0m             )\n\u001B[1;32m   1049\u001B[0m         \u001B[0;31m# error: Too many arguments for \"ParserBase\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1050\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mmapping\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mengine\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mf\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0moptions\u001B[0m\u001B[0;34m)\u001B[0m  \u001B[0;31m# type: ignore[call-arg]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1051\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1052\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_failover_to_python\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/ml/.venv/lib/python3.8/site-packages/pandas/io/parsers.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, src, **kwds)\u001B[0m\n\u001B[1;32m   1865\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1866\u001B[0m         \u001B[0;31m# open handles\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1867\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_open_handles\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msrc\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkwds\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1868\u001B[0m         \u001B[0;32massert\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mhandles\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1869\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mkey\u001B[0m \u001B[0;32min\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0;34m\"storage_options\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"encoding\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"memory_map\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"compression\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/ml/.venv/lib/python3.8/site-packages/pandas/io/parsers.py\u001B[0m in \u001B[0;36m_open_handles\u001B[0;34m(self, src, kwds)\u001B[0m\n\u001B[1;32m   1360\u001B[0m         \u001B[0mLet\u001B[0m \u001B[0mthe\u001B[0m \u001B[0mreaders\u001B[0m \u001B[0mopen\u001B[0m \u001B[0mIOHanldes\u001B[0m \u001B[0mafter\u001B[0m \u001B[0mthey\u001B[0m \u001B[0mare\u001B[0m \u001B[0mdone\u001B[0m \u001B[0;32mwith\u001B[0m \u001B[0mtheir\u001B[0m \u001B[0mpotential\u001B[0m \u001B[0mraises\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1361\u001B[0m         \"\"\"\n\u001B[0;32m-> 1362\u001B[0;31m         self.handles = get_handle(\n\u001B[0m\u001B[1;32m   1363\u001B[0m             \u001B[0msrc\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1364\u001B[0m             \u001B[0;34m\"r\"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/ml/.venv/lib/python3.8/site-packages/pandas/io/common.py\u001B[0m in \u001B[0;36mget_handle\u001B[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001B[0m\n\u001B[1;32m    606\u001B[0m         \u001B[0;31m# ZIP Compression\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    607\u001B[0m         \u001B[0;32melif\u001B[0m \u001B[0mcompression\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;34m\"zip\"\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 608\u001B[0;31m             \u001B[0mhandle\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_BytesZipFile\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mhandle\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mioargs\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmode\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mcompression_args\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    609\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mhandle\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmode\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;34m\"r\"\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    610\u001B[0m                 \u001B[0mhandles\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mhandle\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/ml/.venv/lib/python3.8/site-packages/pandas/io/common.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, file, mode, archive_name, **kwargs)\u001B[0m\n\u001B[1;32m    718\u001B[0m         \u001B[0mkwargs_zip\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mupdate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    719\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 720\u001B[0;31m         \u001B[0msuper\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__init__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfile\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmode\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs_zip\u001B[0m\u001B[0;34m)\u001B[0m  \u001B[0;31m# type: ignore[arg-type]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    721\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    722\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mwrite\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdata\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/lib/python3.8/zipfile.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, file, mode, compression, allowZip64, compresslevel, strict_timestamps)\u001B[0m\n\u001B[1;32m   1249\u001B[0m             \u001B[0;32mwhile\u001B[0m \u001B[0;32mTrue\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1250\u001B[0m                 \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1251\u001B[0;31m                     \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfp\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mio\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mopen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfile\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfilemode\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1252\u001B[0m                 \u001B[0;32mexcept\u001B[0m \u001B[0mOSError\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1253\u001B[0m                     \u001B[0;32mif\u001B[0m \u001B[0mfilemode\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mmodeDict\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'File2.zip'"
     ]
    }
   ],
   "source": [
    "fn = r'File2.zip'\n",
    "df = pd.read_csv(fn, sep='|', encoding='cp1252')\n",
    "df.columns\n",
    "df.shape\n",
    "\n",
    "# date_issued_bank\n",
    "d = df['date_issued_bank'].astype(int).astype(str)\n",
    "df['date_issued_bank'] = pd.to_datetime(d)\n",
    "\n",
    "# date_birth\n",
    "d = df['date_birth'].astype(int).astype(str)\n",
    "df['date_birth'] = pd.to_datetime(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# fix columns types\n",
    "df['NUMCLI'] = df['NUMCLI'].astype(np.int64)\n",
    "df['TOP_SALARIE'] = df['TOP_SALARIE'].astype(np.uint8)\n",
    "df['AGE'] = df['AGE'].astype(np.uint8)\n",
    "df['REVENU'] = df['REVENU'].astype(np.uint32)\n",
    "\n",
    "df['Credit_card'] = df['Credit_card'].astype(np.uint8)\n",
    "df['Savings'] = df['Savings'].astype(np.uint8)\n",
    "df['Morgage'] = df['Morgage'].astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.head(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Input data\n",
    "\n",
    "**X** - vector of available features about clients :\n",
    "* 'AGE' - age of customer in years (numeric)\n",
    "* 'NUMCLI' - (numeric)\n",
    "* 'REVENU' - (numeric)\n",
    "* 'CodPostal' - (category)\n",
    "* 'NB_child' - (numeric)\n",
    "* 'Nationality' - (category)\n",
    "* 'date_issued_bank' - skipped\n",
    "* 'date_birth' - skipped\n",
    "* 'Gender' - (category)\n",
    "* 'Marital_status' - (category)\n",
    "* 'Typ_residency' - (category)\n",
    "* 'TOP_SALARIE' - (category)\n",
    "\n",
    "## Prediction\n",
    "**y** - values we want to predict 0/1: 'Credit_card', 'Savings', 'Morgage'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y = ['Credit_card', 'Savings', 'Morgage']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predicted values are imbalanced (roughly 93% of 0s, 7% of 1s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "bins=26\n",
    "last_date = df['date_birth'].max()\n",
    "df['age_days'] = (last_date - df['date_birth']).dt.days\n",
    "df['age_days'].isna().sum() # no nulls, ok\n",
    "df['age_days_qcut'] = pd.qcut(df['age_days'], bins, labels=False)\n",
    "df['age_days_qcut'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "bins=12\n",
    "df['REVENU'].dtype\n",
    "df['REVENU'] = df['REVENU'].astype(np.uint32)\n",
    "df['REVENU'].isna().sum() # no nulls, ok\n",
    "df['REVENU_qcut'] = pd.qcut(df['REVENU'], bins, labels=False)\n",
    "df['REVENU_qcut'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df['NB_child'].isna().sum()  # some nulls\n",
    "df['NB_child'].mean() # 0.85\n",
    "df['NB_child'].max() # 9\n",
    "\n",
    "df['NB_child'].fillna(1, inplace=True)\n",
    "df['NB_child'] = df['NB_child'].clip(upper=4)\n",
    "df['NB_child'] = df['NB_child'].astype(np.int8)\n",
    "df['NB_child'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "bins=26\n",
    "df['seniority_days'] = (df['date_issued_bank'].max() - df['date_issued_bank']).dt.days\n",
    "df['seniority_days'].isna().sum() # no nulls, ok\n",
    "\n",
    "q01 = df['seniority_days'].quantile(0.01)\n",
    "q99 = df['seniority_days'].quantile(0.99)\n",
    "df['seniority_days'] = df['seniority_days'].clip(q01, q99)\n",
    "\n",
    "m = df['seniority_days']<q99\n",
    "df['seniority_days_qcut'] = pd.qcut(df.loc[m, 'seniority_days'], bins, labels=False)\n",
    "df.loc[-m , 'seniority_days_qcut'] = df['seniority_days_qcut'].max()+1 # last bin\n",
    "\n",
    "df['seniority_days_qcut'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['Nationality'].isna(), ['Nationality']] = 'unk'\n",
    "df.loc[df['Nationality']=='D', ['Nationality']] = 'oth'\n",
    "df.loc[df['Nationality']=='C', ['Nationality']] = 'oth'\n",
    "df.loc[df['Nationality']=='0', ['Nationality']] = 'oth'\n",
    "df['Nationality'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# prepare X vector\n",
    "\n",
    "# CodPostal, 10 most common codes, all remaining into single category\n",
    "len(df['CodPostal'].value_counts())\n",
    "sum(df['CodPostal'].isna())\n",
    "df['CodPostal'].astype(str)\n",
    "df['CodPostal'].max()\n",
    "df['CodPostal'].min()\n",
    "\n",
    "fq = df['CodPostal'].value_counts()\n",
    "fq.head(10).index\n",
    "m = -df['CodPostal'].isin(fq.head(10).index)\n",
    "df.loc[m, 'CodPostal'] = 0\n",
    "#df['CodPostal'] = df['CodPostal'].astype('category').cat.codes.astype(str)\n",
    "df['CodPostal'] = df['CodPostal'].astype(int).astype(str)\n",
    "df['CodPostal'].value_counts()\n",
    "pd.get_dummies(df['CodPostal'])\n",
    "\n",
    "# Gender, category\n",
    "d = df['Gender'].value_counts()\n",
    "# MR=mr.Monsieur MME=mrs.Madame MLE=miss.Mademoiselle\n",
    "df['Gender'] = df['Gender'].map({'MR':'M', 'MME':'F', 'MLE':'F'})\n",
    "\n",
    "# Marital_status, category\n",
    "df['Marital_status'].value_counts()\n",
    "df['Marital_status']\n",
    "\n",
    "# Typ_residency\n",
    "df['Typ_residency'].value_counts()\n",
    "\n",
    "# TOP_SALARIE; 0,1\n",
    "df['TOP_SALARIE'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cat_names = ['age_days_qcut', 'REVENU_qcut', 'seniority_days_qcut',\n",
    "             'NB_child', 'TOP_SALARIE',\n",
    "             'Nationality', 'Gender', 'CodPostal', 'Marital_status', 'Typ_residency']\n",
    "dfcat = pd.get_dummies(df[cat_names].astype(str), drop_first=True)\n",
    "\n",
    "dfn = df[[]].copy()\n",
    "\n",
    "dfc = pd.concat([dfn, dfcat], axis=1)\n",
    "\n",
    "dfc.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dfc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CodPostal I take 10 most common values, remaining are changed to 11th category other.\n",
    "\n",
    "NB_child some values are missing, I fill in with mean values.\n",
    "\n",
    "Gender will be only two values: Male, Female\n",
    "\n",
    "All category variables are changed to dummy variables i.e.: 'Nationality', 'Gender', 'CodPostal', 'Marital_status', 'Typ_residency'\n",
    "\n",
    "I add days = date_issued_bank - date_birth\n",
    "\n",
    "All numerical variable will be scaled for use in log-regr."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#%% add scaling\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#dfn_sc = dfn.copy()\n",
    "#scale(dfn)\n",
    "#scaler = StandardScaler()\n",
    "#scaler.fit(dfn)\n",
    "#dfn_sc[dfn.columns] = scaler.transform(dfn)\n",
    "\n",
    "#dfc = pd.concat([dfn_sc, dfcat], axis=1)\n",
    "dfc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "X=dfc.copy()\n",
    "X['_const']=1\n",
    "vif = pd.DataFrame()\n",
    "vif['VIF'] = [variance_inflation_factor(X.values, i) \n",
    "              for i in range(len(X.columns))]\n",
    "vif['variable'] = X.columns\n",
    "vif.iloc[:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VIF if colinearity test. Log-reg should not get colinear variables as input. This does not harm predictions too much. This only makes model unstable (model parameters may become large) and prevents model parameters from having simple interpretation (as odds of events).\n",
    "\n",
    "After I calculate VIF for the varibale set I remove those with VIF>5 (suspected colinearity). Removed: 'Typ_residency_En accession à la propriété', 'Typ_residency_Locataire', 'Typ_residency_Proprietaire'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#%% based on VIF I remove linearily correlated columns\n",
    "dfc.drop(['Typ_residency_En accession à la propriété',\n",
    "          'Typ_residency_Locataire',\n",
    "          'Typ_residency_Proprietaire',\n",
    "         ], axis=1, inplace=True)\n",
    "dfc.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression classifier\n",
    "## Predicion for 'Credit_card'\n",
    "\n",
    "I will train my model by using only 80% of available data and I will use rest for testing (evaluating)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#%% split to train and test\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X = dfc\n",
    "y_names = 'Credit_card', 'Savings', 'Morgage'\n",
    "y = df[y_names[0]]\n",
    "y.value_counts()\n",
    "y[y==0].size/y.size\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I split into train (80%) and test (20%) sets. For now I use y='Credit_card'.\n",
    "\n",
    "I apply LogisticRegression clasifier with regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#%% log regression\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_proba = model.predict_proba(X_test)\n",
    "print(f'Accuracy: {accuracy_score(y_test, y_pred)*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input set is imbalanced. This makes accuracy and confusion matrix not very informative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#%% confusion_matrix plot\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "plot_confusion_matrix(model, X_test, y_test,\n",
    "                      display_labels=('No', 'Yes'), cmap=plt.cm.Blues)\n",
    "mat = confusion_matrix(y_test, y_pred)\n",
    "mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#%% ROC\n",
    "fpr, tpr, thres = roc_curve(y_test, y_pred_proba[:,1])\n",
    "\n",
    "df_roc = pd.DataFrame(dict(fpr=fpr, tpr=tpr, fpr_=fpr))\n",
    "ax = df_roc.plot.line(x='fpr', y='tpr', title=f'auc={auc(fpr, tpr):.4f}')\n",
    "df_roc.plot.line(x='fpr', y='fpr_', ax=ax, grid=True, style='--')\n",
    "ax.set_xlabel(\"False positive rate\")\n",
    "ax.set_ylabel(\"True positive rate\")\n",
    "ax.get_legend().remove()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By offering a Credit_card to 20% of most promising clients, we can expect to reach over 40% of total number of clients interested in Credit_card."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X_test_full = X_test.copy()\n",
    "X_test_full['y_pred_proba'] = y_pred_proba[:,1]\n",
    "X_test_full['y_pred'] = y_pred\n",
    "X_test_full['y_test'] = y_test\n",
    "X_test_full.sort_values('y_pred_proba', ascending=False, inplace=True)\n",
    "df_Credit_card = X_test_full.iloc[:20000]\n",
    "\n",
    "df_Credit_card['y_test'].cumsum().reset_index(drop=True).plot(grid=True)\n",
    "\n",
    "df_Credit_card = df_Credit_card.copy()\n",
    "df_Credit_card['y_pred']=1\n",
    "mat = confusion_matrix(df_Credit_card['y_test'], df_Credit_card['y_pred'])\n",
    "mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By offering a Credit_card to 20000 of most promising clients, we can expect to make 3106 sales. The total sale potential for all 100000 clients is 7325 sales, so we manage to hold over 40% of business by only addressing 20% of clients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will make one more check. I will display log-reg coeffitients for each variable. I observe that Nationality_F, CodPostal_69100 and Marital_status_Inconnu have P-value above 0.05 meaning, that they are not sufficiently informative for this model (and could be removed to simplify model).\n",
    "\n",
    "I also observe that AGE, NB_child and Nationality_X have negative correlation with Credit_card. \n",
    "\n",
    "\n",
    "REVENU, TOP_SALARIE, Nationality_D, CodPostal_13127, Marital_status_Divorcé, Marital_status_Veuf, Typ_residency_Inconnu, Typ_residency_Proprietaire have positive correlation with Credit_card."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "X_train_ = sm.add_constant(X_train)\n",
    "logit_model=sm.Logit(y_train, X_train_)\n",
    "result=logit_model.fit()\n",
    "print(result.summary2())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicion for 'Savings'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X = dfc\n",
    "y_names = 'Credit_card', 'Savings', 'Morgage'\n",
    "y = df[y_names[1]]\n",
    "y.value_counts()\n",
    "y[y==0].size/y.size\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state=42)\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_proba = model.predict_proba(X_test)\n",
    "print(f'Accuracy: {accuracy_score(y_test, y_pred)*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#%% ROC\n",
    "fpr, tpr, thres = roc_curve(y_test, y_pred_proba[:,1])\n",
    "\n",
    "df_roc = pd.DataFrame(dict(fpr=fpr, tpr=tpr, fpr_=fpr))\n",
    "ax = df_roc.plot.line(x='fpr', y='tpr', title=f'auc={auc(fpr, tpr):.4f}')\n",
    "df_roc.plot.line(x='fpr', y='fpr_', ax=ax, grid=True, style='--')\n",
    "ax.set_xlabel(\"False positive rate\")\n",
    "ax.set_ylabel(\"True positive rate\")\n",
    "ax.get_legend().remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X_test_full = X_test.copy()\n",
    "X_test_full['y_pred_proba'] = y_pred_proba[:,1]\n",
    "X_test_full['y_pred'] = y_pred\n",
    "X_test_full['y_test'] = y_test\n",
    "X_test_full.sort_values('y_pred_proba', ascending=False, inplace=True)\n",
    "df_Credit_card = X_test_full.iloc[:20000]\n",
    "\n",
    "df_Credit_card['y_test'].cumsum().reset_index(drop=True).plot(grid=True)\n",
    "\n",
    "df_Credit_card = df_Credit_card.copy()\n",
    "df_Credit_card['y_pred']=1\n",
    "mat = confusion_matrix(df_Credit_card['y_test'], df_Credit_card['y_pred'])\n",
    "mat, X_test_full['y_test'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By offering a Credit_card to 20000 of most promising clients, we can expect to make 1831 sales. The total sale potential for all 100000 clients is 6660 sales. We only retain 27% of business by restricting to 20% of all clients.\n",
    "\n",
    "This is less helpful than with Credit_card."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicion for 'Morgage'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X = dfc\n",
    "y_names = 'Credit_card', 'Savings', 'Morgage'\n",
    "y = df[y_names[2]]\n",
    "y.value_counts()\n",
    "y[y==0].size/y.size\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state=42)\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_proba = model.predict_proba(X_test)\n",
    "print(f'Accuracy: {accuracy_score(y_test, y_pred)*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#%% ROC\n",
    "fpr, tpr, thres = roc_curve(y_test, y_pred_proba[:,1])\n",
    "\n",
    "df_roc = pd.DataFrame(dict(fpr=fpr, tpr=tpr, fpr_=fpr))\n",
    "ax = df_roc.plot.line(x='fpr', y='tpr', title=f'auc={auc(fpr, tpr):.4f}')\n",
    "df_roc.plot.line(x='fpr', y='fpr_', ax=ax, grid=True, style='--')\n",
    "ax.set_xlabel(\"False positive rate\")\n",
    "ax.set_ylabel(\"True positive rate\")\n",
    "ax.get_legend().remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X_test_full = X_test.copy()\n",
    "X_test_full['y_pred_proba'] = y_pred_proba[:,1]\n",
    "X_test_full['y_pred'] = y_pred\n",
    "X_test_full['y_test'] = y_test\n",
    "X_test_full.sort_values('y_pred_proba', ascending=False, inplace=True)\n",
    "df_Credit_card = X_test_full.iloc[:20000]\n",
    "\n",
    "df_Credit_card['y_test'].cumsum().reset_index(drop=True).plot(grid=True)\n",
    "\n",
    "df_Credit_card = df_Credit_card.copy()\n",
    "df_Credit_card['y_pred']=1\n",
    "mat = confusion_matrix(df_Credit_card['y_test'], df_Credit_card['y_pred'])\n",
    "mat, X_test_full['y_test'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By offering a Credit_card to 20000 of most promising clients, we can expect to make 2455 sales. The total sale potential for all 100000 clients is 6010 sales. We only retain 39% of business by restricting to 20% of all clients.\n",
    "\n",
    "This is less helpful than with Credit_card."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RandomForestClassifier\n",
    "\n",
    "I will use all varaibles, including  'Typ_residency_En accession à la propriété', 'Typ_residency_Locataire', 'Typ_residency_Proprietaire' which I removed for log-reg. I am not worried about variables being lineraly dependent when using trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dfc = pd.concat([dfn_sc, dfcat], axis=1)\n",
    "X = dfc\n",
    "y_names = 'Credit_card', 'Savings', 'Morgage'\n",
    "y = df[y_names[1]]\n",
    "y.value_counts()\n",
    "y[y==0].size/y.size\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_proba = model.predict_proba(X_test)\n",
    "print(f'Accuracy: {accuracy_score(y_test, y_pred)*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicion for 'Savings'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#%% ROC\n",
    "fpr, tpr, thres = roc_curve(y_test, y_pred_proba[:,1])\n",
    "\n",
    "df_roc = pd.DataFrame(dict(fpr=fpr, tpr=tpr, fpr_=fpr))\n",
    "ax = df_roc.plot.line(x='fpr', y='tpr', title=f'auc={auc(fpr, tpr):.4f}')\n",
    "df_roc.plot.line(x='fpr', y='fpr_', ax=ax, grid=True, style='--')\n",
    "ax.set_xlabel(\"False positive rate\")\n",
    "ax.set_ylabel(\"True positive rate\")\n",
    "ax.get_legend().remove()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# xgboost classifier\n",
    "## Predicion for 'Savings'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest =  xgb.DMatrix(X_test, label=y_test)\n",
    "evallist = [(dtest, 'eval'), (dtrain, 'train')]\n",
    "num_round = 100\n",
    "param = {'max_depth': 6, 'num_class': 2, 'eta': 0.3, 'objective': 'multi:softprob' }\n",
    "\n",
    "bst = xgb.train(param, dtrain, num_round)\n",
    "y_pred_prob = bst.predict(dtest)\n",
    "y_pred = np.asarray([np.argmax(n) for n in y_pred_prob])\n",
    "print(f'Accuracy: {accuracy_score(y_test, y_pred)*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#%% ROC\n",
    "fpr, tpr, thres = roc_curve(y_test, y_pred_proba[:,1])\n",
    "\n",
    "df_roc = pd.DataFrame(dict(fpr=fpr, tpr=tpr, fpr_=fpr))\n",
    "ax = df_roc.plot.line(x='fpr', y='tpr', title=f'auc={auc(fpr, tpr):.4f}')\n",
    "df_roc.plot.line(x='fpr', y='fpr_', ax=ax, grid=True, style='--')\n",
    "ax.set_xlabel(\"False positive rate\")\n",
    "ax.set_ylabel(\"True positive rate\")\n",
    "ax.get_legend().remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "vi = bst.get_score(importance_type='gain')\n",
    "dfg = pd.DataFrame(vi.items(), columns = ('feature', 'importance'))\n",
    "dfg.sort_values('importance', ascending=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "Variables 'Credit_card', 'Morgage' are easier to predict. Variable 'Savings' is more difficult. I am disappointed by random forest classifiers (both sklearn and xgboost implementations). And at the same time impressed by simple sklearn Logistic Regression.\n",
    "\n",
    "I have not used variables 'date_issued_bank' and 'date_birth'.\n",
    "\n",
    "Variable  TOP_SALARIE is most important predictor. But it does not look right to me. It is binary 0/1 variable. What is it? Is it real/artificial? How is it obtained?\n",
    "\n",
    "### todo:\n",
    "- equalize 0s and 1s in input set (generate fake records)\n",
    "- are we allowed to use 'Credit_card', 'Morgage' as input to predicting 'Savings'?\n",
    "- check covariance 'Credit_card', 'Morgage' and 'Savings'\n",
    "\n",
    "### questions:\n",
    "- how to assess usefulness (commercial value) of a model?\n",
    "- is there a known price of fp and fn? They are probably different. How costly is failed sale attempt? How costly is lost customer we failed make sale to?\n",
    "\n",
    "### things I tried with minor or no effect:\n",
    "- add higher order values of features (REVENU^2, NB_child^2, age^2)\n",
    "- use SVM with rbf kernel (training takes long, about 5 minutes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call 1 summary\n",
    "2021-01-21\n",
    "\n",
    "- remove duplicates\n",
    "- manually remove observations with salary below 1000euro\n",
    "- manually restric to just ages 25-55\n",
    "- add variable seniority: date_issued_bank - today() (in years)\n",
    "- change all variables to categorical using quartiles"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}